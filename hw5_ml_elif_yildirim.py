# -*- coding: utf-8 -*-
"""HW5-ML-Elif-Yildirim.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mmDC5gjIJNsOzv4XgGryI81ukicbQJ3R

# CONCEPTUAL QUESTION 3

### a
When performing k-fold cross-validation, the dataset is split into k-number of equal "folds". A fold is a subset of the data. The model iterates through the data k-number of times. Each time, 1 fold is excluded from the training data, and is used as test data instead. Every one of the k-number of times the model iterates through the data, the 1 fold being used as the test data is alternated. For each iteration, the model calculates performance metrics. In the end, these metrics are averaged out, and you receive the model evaluation using k-folds cross-validation.

###b

#### validation set

The disadvantage of the k-folds method compared to the validation set approach, is that it requires more resources all around. The model is being trained a k-number of times instead of once, so it is using more computational, financial, and even labor resources.
The advantages of the k-folds method is that it produces more reliable model evaluations than the validation set approach. During this method, the entire dataset is used for training, leading to reduced variance of the model and better performance metrics. With the validation set approach, if the split is unlucky one either side, it's performance may be weak.

#### loocv

LOOCV is a branch of k-folds where k = n, the sample size. This means that every single datapoint will be used as test data at some point, while the n-1 datapoints are used as training data. LOOCV method is much more expensive than the k-folds method. For very large datasets, this method is impractical, making k-folds more efficient. However, because LOOCV uses the entire dataset (minus one point) as training data, this can result in it having a lower bias than k-fold because less data is omitted for testing.

#APPLIED QUESTION 5
"""

pip install ISLP

#LOGISTIC REGRESSION
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from ISLP import load_data

random_seed = np.random.seed(1515)

df = load_data("Default")
#print(df.head())

df['default'] = df['default'].map({'Yes': 1, 'No': 0})

#print(df.info())
X = df[['income', 'balance']]
y = df['default']

model1 = LogisticRegression()
model1.fit(X, y)
print(model1.intercept_)
print(model1.coef_)

#VALIDATION
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=random_seed)

print({X_train.shape[0]})
print({X_val.shape[0]})
model2 = LogisticRegression()
model2.fit(X_train, y_train)

y_prob = model2.predict_proba(X_val)[:, 1]

y_pred1 = (y_prob > 0.5).astype(int)

#Testing
print(y_pred1[:100])

#Validation set error

validation_error = np.mean(y_pred1 != y_val)

print(f"(Misclassified Rate): {validation_error:.4f}")

#accuracy
accuracy = accuracy_score(y_val, y_pred1)
print(f"Accuracy: {accuracy}")

#Repeating 3 times

seeds = [15, 55, 555]
validation_errors = []

for seed in seeds:
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=seed)

    model = LogisticRegression()
    model.fit(X_train, y_train)

    y_prob = model.predict_proba(X_val)[:, 1]
    y_pred = (y_prob > 0.5).astype(int)
    validation_error = np.mean(y_pred != y_val)
    validation_errors.append(validation_error)

    print(f"Validation Set Error: {validation_error}")

"""When I repeat this process 3 times, splitting my data differently, I got three validation error rates between 0.0250 - 0.0303. The error is roughly consistent across the three trials, translating to 2.5-3%. These low error rates indicate that the model is distinguishing between default/nondefault well. We can infer that a logistic model was a good choice for this data, and that income and balance are significant predictors of default status."""

#Using dummy variable student

df['student'] = df['student'].map({'Yes': 1, 'No': 0})
X2 = df[['income', 'balance', 'student']]
y2 = df['default']

X_train2, X_val2, y_train2, y_val2 = train_test_split(X2, y2, test_size=0.3, random_state=15)

model3 = LogisticRegression()
model3.fit(X_train2, y_train2)

y_prob_student = model3.predict_proba(X_val2)[:, 1]
y_pred_student = (y_prob_student > 0.5).astype(int)
validation_error_student = np.mean(y_pred_student != y_val2)
print({validation_error_student})

"""When we used student as a dummy variable, we get a validation error of ~ 0.031 or 3.1%. This is barely any higher than our previous validation errors, meaning adding student did not improve the performance of our model. This means that unlike income or balance, student is not a useful predictor in determining default status."""